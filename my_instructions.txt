I came across problems trying to install libunwind while trying to install gperftools. I found this Chinese blog that helped me compile libunwind: https://blog.csdn.net/qq_21407811/article/details/110946943

The custom gperftools repo has something in its README that maybe we have to pay attention to later ("Building the library" and "Linker script".) There seem to be two options: building the baseline library without any Mallacc instructions, or building the library with all Mallacc instructions enabled. Perhaps we should use the first option to simulate un-accelerated mallocs.

Previously got a bunch of errors when trying to compile XIOSim using Bazel. I ran the commands
`bazel test xiosim:/...:all'
`bazel build xiosim/...:all'
and tried to build everything inside the xiosim directory and then ran `bazel build --copt=Wno-error :xiosim" and stuff seemingly worked?

------------------------------------------------------------------------------------------------------------------------
export XIOSIM_TREE=/usr0/home/nfeng/Documents/Courses/15-712/XIOSim
export XIOSIM_INSTALL=/usr0/home/nfeng/Documents/Courses/15-712/XIOSim/bazel-bin
export TARGET_ARCH=k8
export XIOSIM_LOADER_LIB=

sudo XIOSIM_TREE=/usr0/home/nfeng/Documents/Courses/15-712/XIOSim XIOSIM_INSTALL=/usr0/home/nfeng/Documents/Courses/15-712/XIOSim/bazel-bin TARGET_ARCH=k8 python run_experiments.py

------------------------------------------------------------------------------------------------------------------------
Differences made in TCMalloc (gperftools) to simulate Mallacc instructions.

src/common.h
src/libc_override_glibc.h
src/linked_list.h
src/sampler.cc
src/sampler.h

src/tcmalloc.cc
src/tcmalloc.h
src/tcmalloc.ld (new file)
src/thread_cache.cc
src/thread_cache.h

------------------------------------------------------------------------------------------------------------------------
Software to emulate Mallacc is in XIOSim. The malloc cache is called `size_class_cache' and is defined in XIOSim/xiosim/size_class_cache.h.
XIOSim has another data structure called "knobs", defined in 

`xiosim/pintool/feeder.h' defines thread_state_t, which is thread-local state that includes a malloc cache called size_class_cache. Not clear to me yet if the threads in XIOSim refer to just the threads XIOSim uses to split up the simulation work, or if it actually correponds to threads created by the calling program... to simulate a single Mallacc for multiple threads, maybe can change so that each thread_state_t references the same global size_class_cache.

`xiosim/pintool/tcm_hooks/tcm_size_class.cpp' seems like it contains implementation of how size_class_cache handles size lookups.

`xiosim/config/H.cfg' contains some initial parameters for the malloc cache, including its size. Set at 5 initially; let's use 32 for baseline experiments, which is twice as much as what the Mallacc paper determined as sufficient for most workloads (16.) (H.cfg stands for Haswell-class Intel Core i7 processor; likewise, A.cfg contains configuration parameters for an Atom class Intel processor, and N.cfg for Nehalem-class Intel Core i7 processor. The Mallacc paper, as far as I know, just uses Haswell-class, so makes sense all Mallacc-related parameters are only there.)

------------------------------------------------------------------------------------------------------------------------
To plot cache_size_sweep data, first need to place data in the appropriate directory. `parse_results.py' assumes that file hierarchy is as follows:

malloc_out
|	
└---cache_size
	|
	└---benchmark
		|
		└---opt ("baseline, realistic")
			|
 			└---run (0)
 				|
				└---filename

Then need to call parse_results.py with the cache_sweep flag set:

python parse_results.py read --results-dir ../../malloc_out --db speedup.db --is-cache-sweep


To transfer ownership of output data from sudo to nfeng:
sudo chown -R nfeng [malloc_out]
------------------------------------------------------------------------------------------------------------------------
To run my own parsing code: In mallacc/my_scripts, run

```
python fast_path_data.py [mode] --db ../scripts/speedup.db
```
where mode = {speedup, hit-rates, cache-sweep}
